# app/dao/log_dao.py
# --------------------------------------------------------------------------
#  Data‑access object (DAO) for the Logs collection
#  Uses Motor (async MongoDB driver) via `log_collection` helper.
#  NOTE: Every document’s _id is the deterministic string
#        "<agent_id>:<channel>:<record_id>" generated by the user agent.
# --------------------------------------------------------------------------
from typing import List, Optional

from bson import ObjectId               # Needed only if you later insert manual docs
from motor.motor_asyncio import AsyncIOMotorCollection
from pymongo import DESCENDING
from pymongo.errors import DuplicateKeyError, BulkWriteError

from app.db.mongodb import log_collection                # type: AsyncIOMotorCollection
from app.models.log_model import LogEntry

# ────────── Insert helpers ────────────────────────────────────────────────

class LogDAO:
    """
    DAO encapsulates every DB hit so routers / services stay clean.
    """

    # ------------- single insert ------------------------------------------------
    @staticmethod
    async def add_log(log: LogEntry) -> str | None:
        """
        Insert ONE log document.

        Returns:
            the _id if newly inserted
            None       if duplicate (already exists)
        """
        doc = log.dict(by_alias=True)

        try:
            await log_collection.insert_one(doc)
            return doc["_id"]
        except DuplicateKeyError:
            # Duplicate is fine – user agent guarantees "exactly once"
            return None

    # ------------- bulk insert --------------------------------------------------
    @staticmethod
    async def add_logs_bulk(logs: List[LogEntry]) -> int:
        """
        Insert MANY documents with `ordered=False` so duplicates are skipped.

        Returns:
            Number of *new* docs inserted.
        """
        if not logs:
            return 0

        docs = [log.dict(by_alias=True) for log in logs]

        try:
            result = await log_collection.insert_many(docs, ordered=False)
            return len(result.inserted_ids)
        except BulkWriteError as exc:
            # Count only successful inserts (ignoring dup errors)
            return exc.details.get("nInserted", 0)

    # ────────── Update / Delete ────────────────────────────────────────────

    @staticmethod
    async def update_log(log_id: str, changes: dict) -> bool:
        """
        Update selected fields of a log.
        Returns True if a document was actually modified.
        """
        outcome = await log_collection.update_one({"_id": log_id}, {"$set": changes})
        return outcome.modified_count > 0

    @staticmethod
    async def delete_log(log_id: str) -> bool:
        outcome = await log_collection.delete_one({"_id": log_id})
        return outcome.deleted_count > 0

    # ────────── Getters ────────────────────────────────────────────────────

    @staticmethod
    async def get_log_by_id(log_id: str) -> Optional[LogEntry]:
        doc = await log_collection.find_one({"_id": log_id})
        return LogEntry(**doc) if doc else None

    @staticmethod
    async def get_logs_by_filter(
        agent_id: Optional[str] = None,
        channel:  Optional[str] = None,
        level:    Optional[str] = None,
        skip: int = 0,
        limit: int = 100,
    ) -> List[LogEntry]:
        """
        Flexible finder used by GET /logs.
        """
        query: dict = {}
        if agent_id:
            query["agent_id"] = agent_id
        if channel:
            query["channel"] = channel
        if level:
            query["level"] = level

        cursor = (
            log_collection
            .find(query)
            .sort("timestamp", DESCENDING)
            .skip(skip)
            .limit(limit)
        )
        return [LogEntry(**doc) async for doc in cursor]

    @staticmethod
    async def get_all_logs(skip: int = 0, limit: int = 300) -> List[LogEntry]:
        cursor = (
            log_collection
            .find()
            .sort("timestamp", DESCENDING)
            .skip(skip)
            .limit(limit)
        )
        return [LogEntry(**doc) async for doc in cursor]

# --------------------------------------------------------------------------
# Optional utility – quick vacuum for dev / tests
# --------------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio

    async def _wipe():
        deleted = await log_collection.delete_many({})
        print(f"✂️  Deleted {deleted.deleted_count} documents")

    asyncio.run(_wipe())
