# app/dao/log_dao.py
# --------------------------------------------------------------------------
#  Data‑access object (DAO) for the Logs collection
#  Uses Motor (async MongoDB driver) via `log_collection` helper.
#  NOTE: Every document’s _id is the deterministic string
#        "<agent_id>:<channel>:<record_id>" generated by the user agent.
# --------------------------------------------------------------------------
from typing import List, Optional

from bson import ObjectId               # Needed only if you later insert manual docs
from motor.motor_asyncio import AsyncIOMotorCollection
from pymongo import DESCENDING
from pymongo.errors import DuplicateKeyError, BulkWriteError

from app.db.mongodb import log_collection                # type: AsyncIOMotorCollection
from app.models.full_log import FullLogEntry
from app.models.log_model import LogEntry
from app.models.log_update_model import LogUpdate


# ────────── Insert helpers ────────────────────────────────────────────────

class LogDAO:
    """
    DAO encapsulates every DB hit so routers / services stay clean.
    """

    # ------------- single insert ------------------------------------------------
    @staticmethod
    async def add_log(log: FullLogEntry) -> str | None:
        """
        Insert ONE enriched log document into MongoDB.
        """
        doc = log.dict(by_alias=True)

        try:
            await log_collection.insert_one(doc)
            return doc["_id"]
        except DuplicateKeyError:
            # Duplicate is fine; it won't be re-inserted
            return None

    # ------------- bulk insert --------------------------------------------------
    @staticmethod
    async def add_logs_bulk(logs: List[FullLogEntry]) -> int:
        """
        Insert MANY documents with `ordered=False` so duplicates are skipped.

        Returns:
            Number of *new* docs inserted.
        """
        if not logs:
            return 0

        docs = [log.dict(by_alias=True) for log in logs]

        try:
            result = await log_collection.insert_many(docs, ordered=False)
            return len(result.inserted_ids)
        except BulkWriteError as exc:
            # Count only successful inserts (ignoring dup errors)
            return exc.details.get("nInserted", 0)

    # ────────── Update / Delete ────────────────────────────────────────────

    @staticmethod
    async def update_log(log_id: str, changes: dict) -> bool:
        """
        Update selected fields of a log.
        Returns True if a document was actually modified.
        """
        outcome = await log_collection.update_one({"_id": log_id}, {"$set": changes})
        return outcome.modified_count > 0

    @staticmethod
    async def delete_log(log_id: str) -> bool:
        outcome = await log_collection.delete_one({"_id": log_id})
        return outcome.deleted_count > 0

    # ────────── Getters ────────────────────────────────────────────────────

    @staticmethod
    async def get_log_by_id(log_id: str) -> Optional[FullLogEntry]:
        doc = await log_collection.find_one({"_id": log_id})
        return FullLogEntry(**doc) if doc else None

    @staticmethod
    async def get_logs_by_filter(
        agent_id: Optional[str] = None,
        channel:  Optional[str] = None,
        level:    Optional[str] = None,
        skip: int = 0,
        limit: int = 100,
    ) -> List[FullLogEntry]:
        """
        Flexible finder used by GET /logs.
        """
        query: dict = {}
        if agent_id:
            query["agent_id"] = agent_id
        if channel:
            query["channel"] = channel
        if level:
            query["level"] = level

        cursor = (
            log_collection.find(query)
            .sort("timestamp", DESCENDING)
            .skip(skip)
            .limit(limit)
        )
        results = []
        async for doc in cursor:
            # Convert `_id` to string but don't remove the field
            doc["_id"] = str(doc["_id"])
            results.append(FullLogEntry(**doc))
        return results

    @staticmethod
    async def get_all_logs(skip: int = 0, limit: int = 300) -> List[FullLogEntry]:
        cursor = (
            log_collection.find()
            .sort("timestamp", DESCENDING)
            .skip(skip)
            .limit(limit)
        )
        results = []
        async for doc in cursor:
            # Convert `_id` to string but don't remove the field
            doc["_id"] = str(doc["_id"])
            results.append(FullLogEntry(**doc))
        return results

# --------------------------------------------------------------------------
# Optional utility – quick vacuum for dev / tests
# --------------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio

    async def _wipe():
        deleted = await log_collection.delete_many({})
        print(f"✂️  Deleted {deleted.deleted_count} documents")

    asyncio.run(_wipe())